# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1igqPyM6VKvrx609DhO1l9VEB7IEF1m0-
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

import tensorflow as tf
from tensorflow.keras import layers, Model, Input

import statsmodels.api as sm

np.random.seed(42)

T = 6000

t = np.arange(T)

x1 = np.sin(0.02*t) + np.random.normal(0,0.1,T)
x2 = np.cos(0.015*t) + 0.3*x1 + np.random.normal(0,0.1,T)
x3 = 0.5*np.sin(0.04*t)*x1 + 0.2*np.random.normal(0,1,T)
x4 = 0.3*x2 + 0.3*x3 + np.random.normal(0,0.2,T)
x5 = (x1*x3) + np.random.normal(0,0.1,T)  # nonlinear interaction

df = pd.DataFrame([x1,x2,x3,x4,x5]).T
df.columns = ['x1','x2','x3','x4','x5']

df.plot(figsize=(12,5))
plt.title("Generated Multivariate Nonlinear Time Series")
plt.show()

def create_dataset(data, look_back=30):
    X, y = [], []
    for i in range(len(data)-look_back):
        X.append(data[i:i+look_back])
        y.append(data[i+look_back, 0])      # forecast x1
    return np.array(X), np.array(y)

scaler = MinMaxScaler()
scaled = scaler.fit_transform(df)

look_back = 30
X, y = create_dataset(scaled, look_back)

train_size = int(0.8*len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

def create_dataset(data, look_back=30):
    X, y = [], []
    for i in range(len(data)-look_back):
        X.append(data[i:i+look_back])
        y.append(data[i+look_back, 0])      # forecast x1
    return np.array(X), np.array(y)

scaler = MinMaxScaler()
scaled = scaler.fit_transform(df)

look_back = 30
X, y = create_dataset(scaled, look_back)

train_size = int(0.8*len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

sarimax_model = sm.tsa.SARIMAX(df['x1'], order=(2,0,2))
sarimax_result = sarimax_model.fit(disp=False)

sarimax_pred = sarimax_result.predict(start=train_size+look_back, end=T-1)
baseline_rmse = np.sqrt(mean_squared_error(df['x1'].iloc[train_size+look_back:], sarimax_pred))

print("SARIMAX RMSE:", baseline_rmse)

lstm_model = tf.keras.Sequential([
    layers.LSTM(64, return_sequences=False, input_shape=(look_back, 5)),
    layers.Dense(1)
])

lstm_model.compile(optimizer='adam', loss='mse')

history = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

class BahdanauAttention(layers.Layer):
    def _init_(self, units):
        super()._init_()
        self.W1 = layers.Dense(units)
        self.W2 = layers.Dense(units)
        self.V = layers.Dense(1)

    def call(self, values, query):
        query = tf.expand_dims(query, 1)
        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query)))
        attention_weights = tf.nn.softmax(score, axis=1)
        context = attention_weights * values
        context = tf.reduce_sum(context, axis=1)
        return context, attention_weights

input_layer = Input(shape=(look_back, 5))

lstm_out = layers.LSTM(64, return_sequences=True)(input_layer)
lstm_last = layers.LSTM(32, return_sequences=False)(lstm_out)

attention = BahdanauAttention(32)
context_vector, att_weights = attention(lstm_out, lstm_last)

dense1 = layers.Dense(32, activation='relu')(context_vector)
output = layers.Dense(1)(dense1)

att_model = Model(inputs=input_layer, outputs=output)
att_model.compile(optimizer='adam', loss='mse')
att_model.summary()

att_history = att_model.fit(X_train, y_train, epochs=20, batch_size=32,
                            validation_split=0.1, verbose=1)

pred = att_model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, pred))
mae = mean_absolute_error(y_test, pred)
mape = np.mean(np.abs((y_test - pred.flatten()) / y_test)) * 100

print("Attention LSTM RMSE:", rmse)
print("MAE:", mae)
print("MAPE:", mape)

att_output_model = Model(inputs=att_model.input, outputs=att_model.layers[3].output)

sample = X_test[100:101]
context_vec, att_w = att_output_model.predict(sample)

plt.figure(figsize=(10,4))
plt.stem(att_w.flatten())
plt.title("Attention Weights Across Time Steps")
plt.xlabel("Time Step")
plt.ylabel("Weight")
plt.show()

def train_with_params(units):
    model = tf.keras.Sequential([
        layers.LSTM(units, return_sequences=True, input_shape=(look_back,5)),
        layers.LSTM(units),
        layers.Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)
    pred = model.predict(X_test)
    return np.sqrt(mean_squared_error(y_test, pred))

for u in [32, 64, 128]:
    print("Units:", u, "RMSE:", train_with_params(u))